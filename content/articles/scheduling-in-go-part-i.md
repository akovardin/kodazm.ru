+++
date = "2018-11-12T20:53:09+03:00"
draft = true
title = "Планировщик в Go - Часть первая I"
+++

Перевод статьи "[Scheduling In Go - Part I](https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html)".

### Введение

Дизайн и поведение планировщика в Go позволяют вашим многопоточным программам быть более производительными. Планировщик Go очень эффективно работает с операционной системой и именно это делает его таким крутым. Если вы хотите добиться производительности, то вам нужно писать многопоточные программы которые будут эффективно работать с планировщиком. А для этого нужно понимать как он устроен и как он взаимодействует с ОС.

В этом цикле статей я сфокусируюсь на высокоуровневых механизмах и семантиках планировщика. Постараюсь как можно детальнее описать как все устроено. Опираясь на эти знания вы сможете применять более подходящие решения при написании многопоточных программ. Несмотря на большой выбор различных решений, существует ряд базовых механизмов которые вам нужно знать.

### Планировщик OC

Планировщик в операционной системе это большой и сложная часть программного обеспечения. Они должны учитывать какое оборудование используется и как оно настроено. Используются ли многопроцессорные и многоядерные системы, [кеши процессора, NUMA](http://frankdenneman.nl/2016/07/06/introduction-2016-numa-deep-dive-series) и многое другое. Без этих знаний об оборудовании планировщик не смог бы эффективно работать. К счастью, у нас есть возможность разобраться с принципами работы планировщика без погружения в такие дебри.

Ваша программа это набор машинных инструкций, которые должны быть выполнены одна за другой. Для этого ОС использует потоки. Задача потока - выстраивать очередь закрепленных за ним инструкций. Выполнение продолжается до тех пор пока все инструкции ен выполнятся. Поэтому я называю потоки "путь выполнения".

Каждая программа, которую вы запускаете, создает процесс и каждый процесс запускает начальный поток. Потоки могут запускать другие потоки. Все эти потоки работают независимо и планировщик реализует это на уровне потоков не зависимо от запущенных процессов. Потоки могут работать конкурентно(на одном ядре) или параллельно(каждый поток работает на своем ядре). Потоки следят за своим состоянием чтобы обеспечивать безопасное и независимое выполнение инструкций.

Планировщик ОС следит чтобы ядра не простаивали когда есть потоки которые могли бы работать. Кроме того, создается впечатление что все потоки работают параллельно. Для создания такой иллюзии параллельной работы, планировщик должен запускать потоки с высоким приоритетом чаще чем потоки с низким приоритетом. Потоки с высоким приоритетом не должны "голодать" и недополучать процессорное время. Планировщик должен стараться минимизировать время переключения насколько это возможно. 

Алгоритм такого планировщика должен учитывать множество нюансов. К счастью, за несколько десятилетий мы накопили достаточно опыта который можем использовать. Чтобы понять как именно этот алгоритм должен работать, давайте опишем несколько базовых концепций.

### Выполнение инструкций

Программный счетчик([program counter](https://en.wikipedia.org/wiki/Program_counter) или PC) который еще называют указателем на инструкцию(instruction pointer или IP) хранит позицию следующей инструкции.

![](/img/scheduler/1.jpeg)

Если вы хоть раз видели трейс Go программы, то наверняка замечали шестнадцатеричные числа в конце каждой строки. Например `+0x39` и `+0x72` как в коде ниже:

```
goroutine 1 [running]:
   main.example(0xc000042748, 0x2, 0x4, 0x106abae, 0x5, 0xa)
       stack_trace/example1/example1.go:13 +0x39                 <- LOOK HERE
   main.main()
       stack_trace/example1/example1.go:8 +0x72                  <- LOOK HERE
```

Это значения PC - смещение относительно начала текущей функции. Значение ` +0x39` указывает на следующую инструкцию, которая будет выполнена в Потоке внутри функции `example` если бы программа не запаниковала бы. Значение `0+x72` указывает на инструкцию внутри функции `main` которой должно передаться управление.

Рассмотрим программу, которая сгенерировала этот стек трейс. 

```go
//https://github.com/ardanlabs/gotraining/blob/master/topics/go/profiling/stack_trace/example1/example1.go

func main() {
    example(make([]string, 2, 4), "hello", 10)
}

func example(slice []string, str string, i int) {
    panic("Want stack trace")
}
```

`+0x39` представляет собой смещение внутри `example` на 57(в десятичной системе) байтов относительно начальной инструкции. Ниже приведен `objdump` функции `example`. 12 инструкция это как раз то что мы ищем. И обратите внимание, что на 11 происходит вызов `panic`

```
$ go tool objdump -S -s "main.example" ./example1
TEXT main.example(SB) stack_trace/example1/example1.go
func example(slice []string, str string, i int) {
  0x104dfa0		65488b0c2530000000	MOVQ GS:0x30, CX
  0x104dfa9		483b6110		CMPQ 0x10(CX), SP
  0x104dfad		762c			JBE 0x104dfdb
  0x104dfaf		4883ec18		SUBQ $0x18, SP
  0x104dfb3		48896c2410		MOVQ BP, 0x10(SP)
  0x104dfb8		488d6c2410		LEAQ 0x10(SP), BP
	panic("Want stack trace")
  0x104dfbd		488d059ca20000	LEAQ runtime.types+41504(SB), AX
  0x104dfc4		48890424		MOVQ AX, 0(SP)
  0x104dfc8		488d05a1870200	LEAQ main.statictmp_0(SB), AX
  0x104dfcf		4889442408		MOVQ AX, 0x8(SP)
  0x104dfd4		e8c735fdff		CALL runtime.gopanic(SB)
  0x104dfd9		0f0b			UD2              <--- LOOK HERE PC(+0x39)
```

Не путайте - PC всегда указывает на следующую инструкцию, не на текущую. Листинг выше это хороший пример набора инструкций которые выполняются в одном потоке последовательно.

### Состояния потока

Еще одни важный концепт - состояние потока, которое определяет как планировщик должен поступать с потоком. Поток может быть в трех состояниях: ожидание, готовность и выполнение.

**Ожидание**. В этом состоянии поток остановлен и ждет чего-то для продолжения своей работы. Это может быть ожидание оборудования(диск или сеть), операционной системы(системные вызовы) или синхронизация(атомики, мютексы). Как правило, такие задержки являются основными причинами плохой производительности.

**Готовность**. Означает что потоку нужно процессорное время, в любое время он готов выполнять инструкции закрепленные за ним. Чем больше потоков находится в этом состоянии, тем дольше каждый из них будет ожидать свободное процессорное время. Такие задержки тоже ведут к ухудшению производительности.

**Выполнение**. Это означает, что инструкции потока выполняется ядром. Приложение работает.

### Типы работы

Существует два типа работы которую может делать поток: CPU-Bound и IO-Bound.

**CPU-Bound**. Когда поток выполняет работу такого типа, он никогда не перейдет в состояние ожидания. Это тип работы, когда постоянно происходят вычисления. Поток который вычисляет число Пи до N знака выполняет CPU-Bound работу.

**IO-Bound**. Это тип работы, когда поток может переходить в состояние ожидания. В этот момент может выполнятся обращение к сети, диску или выполнять системные вызовы. Когда поток обращается с базе данных - это работа типа IO-Bound. Атомики и мютексы, которые могут перевести поток в состояние ожидания, также можно отнести к этому типу работы.

### Переключение контекста

Если работаете на Linux, Mac или Windows то вы работаете на операционной системе с вытесняющим планировщиком. Это означает несколько важных моментов. 

Во-первых, невозможно предсказать, какой поток будет выбран в определенный момент времени. Приоритеты потоков и различные события(такие как получение данных по сети) делают невозможным предсказание кого планировщик выберет в следующий раз.

Во-вторых, при написании кода вы не должны полагаться на поведение, которое вы наблюдали один раз. Не гарантируется, что код будет вести себя также при все последующие разы. Это очень самонадеяно рассчитывать что поведение не поменяется. Вам нужно контролировать синхронизацию и оркестрирование потоков если хотите разбираться в поведении вашего приложения.

Переключение потоков в ядре называется переключением контекстов. Планировщик выталкивает выполняемый поток из ядра и заменяет его потоком в состоянии готовности. Этот новый поток выбранные из очереди переходит в состояние выполнения. Вытолкнутый поток может перейти в состояние готовности(если он может продолжить выполнение в любой момент) или в состояние ожидания(если он вытеснен из-за IO-Bound работы).

Переключение контекста считается дорогой операцией, потому что требуется время на смену потоков в ядре. Время переключения зависит от многих факторов, но в среднем оно занимает ~50 - ~100 наносекунд. Считается, что оборудование позволяет исполнять(в среднем) [12 инструкций за наносекунду](https://www.youtube.com/watch?v=jEG4Qyo_4Bc&feature=youtu.be&t=266). Таким образом, переключение контекста стоит ~600 - ~1200 инструкций.

Если ваша программа делает много IO-Bound работы, то переключение контекста полезно. Когда процессор переходит в состояние ожидания, другой тред в состоянии готовности занимает его место. Процессор не простаивает и постоянно выполняется полезная работа. Это один из важных принципов - не позволяйте процессору простаивать если есть работа(потоки в состоянии готовности).

Если ваша программа в основном выполняет CPU-Bound работу, то переключение контекста может стать ночным кошмаром. Поток выполняет работу, но он постоянно останавливается и заменяется другим потоком. Это ситуация противоположная переключениям при IO-Bound работе.

### Чем меньше, тем больше

В давние времена, когда процессоры имели только одно ядро, планирование не было такой сложной задачей. У вас был только один процессор с одним ядром и только один поток мог выполняться в один момент времени. Идея была простой - определить некоторый промежуток времени([scheduler period](https://lwn.net/Articles/404993/)) и попытаться выполнить все готовые потоки в этот промежуток времени. Никаких проблем: берем этот период и делим его на количество процессов.

Например, у нас есть период в 10мс и два потока. Каждому потоку достается по 5мс. Если потоков 5, то каждый получает по 2мс. Но что будет, если у нас 100 потоков? Отдать каждому потоку по 10мкс не самое лучшее решение - большая часть времени будет тратиться только на переключения контекста.

Модно ограничить минимальный отрезок времени, который будет отдан одному процессу. Например, в примере выше можно выделить каждому потоку по 2мс и для 100 потоков общий период будет занимать 2с. Для 1000 потоков период будет 20с если каждый поток использует все выделенное ему время.

Это очень упрощенный взгляд на вещи. Существует целая куча моментов, которые должен учитывать планировщик. Вам необходимо контролировать количество потоков в вашем приложении. Чем больше потоков и IO-Bound тем непредсказуемей становится поведение вашего приложения. Требуется больше времени для планирования и выполнения.

Вот почему "чем меньше, тем лучше". Меньше потоков в состоянии готовности означает больше времени на переключения контекста и меньше времени на работу каждого потока. Это означает, что зв тоже время выполняется меньше работы.

### Найти баланс

Всегда нужно искать лучший баланс между количеством ядер и количеством потоков, которые позволят работать вашему приложению максимально производительно. Пул потоков будет неплохим решением этой задачи. Во второй части я расскажу что в Go это уже не так необходимо. Go упрощает создание многопоточных приложений.

До программирования на Go, я писал код на C++ и C# на NT. На этой операционной системе использование IOCP (IO Completion Ports) пула потоков было критически важным. Вам нужно учитывать как много потоков пулов вам нужно и сколько потоков вам нужно в каждом пуле. И все это чтобы максиммизировать пропускную способность ядер которые у вас есть.

Когда вы пишите веб-сервис для работы с базой данных, вам нужно использовать магическое число 3 - столько потоков на ядро давало максимальную производительность на NT. Другими словами, 3 потока на ядро минимизировали время на переключение контекста и увеличивали время выполнения. При создании IOCP пула тредов всегда нужно было указывать минимум 1 и максимум 3 потока для каждого ядра.

Когда указываешь по 2 потока на ядро то вся работа выполнялась намного дольше, так как ядра простаивали. Когда указываешь 4 потока, то работа выполняется дольше, потому что много времени уходит переключение контекста. Поэтому число 3 кажется некоторой магической константой при разработке программ под NT.

Что если ваш сервис делает много разнообразной работы? Это создает разные и непостоянные задержки. Вероятно, такая программа создает множество системных событий которые должны быть обработаны. В такой ситуации вам будет сложно найти магическое число которое всегда будет работать как надо. Когда вам приходится использовать пулы потоков, то его конфигурирование может стать непростой задачей.

### Кеши

Доступ к данным в памяти может занимать довольно много времени (~100 - ~300 циклов). Поэтому процессоры и ядра имеют локальный кеш чтобы держать данные ближе к аппаратным потокам. Доступ к данным в кеше значительно быстрее. Чтобы ваше приложение работало быстро, нужно учитывать как ваше приложение обращается к данным. Особенно, если вы пишете многопоточное приложение.

![](/img/scheduler/2.png)

Данные между процессором и памятью обмениваются [кеш линиями](https://www.youtube.com/watch?v=WDIkqP4JbkE). Линии кеша это 64-байтные чанки памяти которыми обменивается процессор и основная память. Каждое ядро имеет копии каждой линии кеша которая ему нужна. Это значит, что оборудование использует [значение семантик](https://www.ardanlabs.com/blog/2017/06/design-philosophy-on-data-and-semantics.html). Частые изменения в памяти могут стать ночным кошмаром для многопоточных программ.

Когда потоки параллельно обращаются к памяти за одними и теми же данными или к очень близко расположенным данным, то они обращаются к одной линии кеша. И каждый поток запущенный на каждом ядре получит свою копию линии кеша.

![](/img/scheduler/3.png)

Если один поток вносит изменения в свою копию линии кеша, то (с помощью магии и оборудования) все копии помечаются как "грязные". Когда поток пытается читать из этих линий, то чтение перенаправляется к памяти для обновления копии.

На двухъядерном процессоре это не такая большая проблем. Но как насчет 32х ядерного процессора и потоков запущенных на каждом ядре, которые постоянно изменяют одни и кеши? Или если у вас два физических процессора к 16ю ядрами на каждом? Это еще хуже, потому что нам придется тратить время на межпроцессорное взаимодействие. Приложение будет перелопачивать память, производительность будет падать и совершенно не будете понимать почему.

Это называется проблемой [когерентности кэша](https://youtu.be/WDIkqP4JbkE). Когда вы пишите многопоточное приложение в котором изменяется общее состояние, то нужно учитывать системы кеширования.

### Сценарии принятия решений в планировщике

Представим, что я попросил вас написать планировщик для ОС опираясь на информацию, которую я написал выше.

Вы запускаете ваше приложение и запускается главный поток на ядре 1. Кака только запускается выполнение инструкций потока, сразу происходит формирование кеша. Предположим, мы хотим запустить еще один поток для конкурентной работы. 
И тут возникает много вопросов:

1. Нужно ли в этот момент переключить контекст с основного потока? Это может благоприятно сказать на производительность(например, для нового потока скорее всего нужны те же данные), но тогда основной поток не доработает весь свой период.
2. Должен ли новый поток ожидать пока ядро не освободиться, т/е пока главный тред не отработает весь отведенный ему период? Поток не работает, но задержка из-за получения данных все еще не так критична.
3. Должен ли поток ждать пока освободится другое ядро? В таком случае придется скопировать кеш, но поток может запуститься быстрее и главный тред продолжит свою работу.

Выглядит весело. Это очень интересные вопросы и планировщик должен учитывать их при принятии решения. Главное на от чего нужно отталкиваться - ядра не должны простаивать. Если поток может работать, то он должен работать.

### Заключение

В первой части мы рассмотрели различные моменты работы планировщика в ОС на которые стоит обращать внимание при написании многопоточных программ. Основные принципы у планировщика в Go сходи с планировщиками в ОС. В следующей части я опишу семантику Go'шного планировщика. И в конце можно будет посмотреть на все это в действии запустив несколько примеров.